# Bulk Crawler - Quick Start

## ğŸš€ Der schnelle Weg

```bash
cd basketball-app
npm run crawl:clubs:bulk
```

**Dauer:** 15-25 Minuten (statt 6-8 Stunden!)

---

## ğŸ¯ Was macht der Bulk-Crawler anders?

### Alte Strategie (Incremental):
```
Verband 1 â†’ Alle Ligen â†’ Team-Details â†’ 20 min
Verband 2 â†’ Alle Ligen â†’ Team-Details â†’ 20 min
Verband 3 â†’ Alle Ligen â†’ Team-Details â†’ 20 min
...
Verband 21 â†’ Alle Ligen â†’ Team-Details â†’ 20 min

Total: ~6-8 Stunden
Problem: Team "167863" wird 5x abgefragt!
```

### Neue Strategie (Bulk):
```
ALLE Ligen auf einmal â†’ Team-Details (dedupliziert) â†’ 15-25 min

Total: ~15-25 Minuten
Vorteil: Team "167863" wird NUR EINMAL abgefragt!
```

---

## ğŸ“Š Strategie-Details

### 1. Lade ALLE Ligen
```javascript
POST /wam/data
{
  "verbandIds": [],  // LEER = ALLE!
  ...
}

â†’ ~2000-3000 Ligen
```

### 2. Batch Processing (10 parallel)
```
Batch 1: Ligen 1-10 â†’ parallel abrufen
Batch 2: Ligen 11-20 â†’ parallel abrufen
...
```

### 3. Deduplizierung
```javascript
Team "167863" in:
- Liga "Bayernliga U10"
- Liga "Deutsche Meisterschaft U10"
- Liga "NBBL"

â†’ Wird NUR EINMAL abgerufen
â†’ Clubdaten werden gemerged
â†’ verbaende: [2, 29, 100]
```

### 4. Periodic Flush (alle 100 Ligen)
```
âœ… 100 Ligen â†’ Save to disk
âœ… 200 Ligen â†’ Save to disk
âœ… 300 Ligen â†’ Save to disk
...

â†’ Bei Crash: Maximal 100 Ligen verloren
```

---

## ğŸ”§ Optionen

### Fast-Mode (empfohlen)
```bash
npm run crawl:clubs:bulk -- --skip-kontakt
```

Ãœberspringt Kontaktdaten â†’ **~20% schneller**

---

## ğŸ“ Output

**Gleiche Struktur wie Incremental Crawler:**

```json
{
  "metadata": {
    "crawledAt": "2025-10-22T16:30:00.000Z",
    "crawlStrategy": "bulk",
    "totalClubs": 1250,
    "totalTeams": 3500,
    "verbaende": [1, 2, 3, ..., 29, 30, 31, 32, 33, 100]
  },
  "clubs": [...]
}
```

**Datei:** `src/shared/data/clubs-germany.json`

---

## ğŸ“Š Performance-Vergleich

| Crawler | Dauer | API Calls | Duplikate |
|---------|-------|-----------|-----------|
| **Incremental (alt)** | 6-8h | ~50.000 | Ja (5x) |
| **Incremental (v3.1)** | 30-45min | ~25.000 | Ja (3x) |
| **Bulk (neu)** | 15-25min | ~10.000 | Nein âœ… |

**Speedup:** 15-20x schneller als alt, 2x schneller als v3.1!

---

## ğŸ§ª Beispiel-Output

```bash
npm run crawl:clubs:bulk

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  BBB Club Crawler - BULK Strategy v1.0    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Output: .../clubs-germany.json
âš¡ Fast-Mode: Kontaktdaten Ã¼bersprungen
ğŸ“¦ Batch Size: 10 parallel
ğŸ’¾ Auto-Save: Alle 100 Ligen

ğŸ“¡ Schritt 1: Lade ALLE Ligen...
âœ… 2847 Ligen geladen

ğŸ”„ Schritt 2: Batch Processing...
ğŸ“Š 100/2847 Ligen (3.5%) | Clubs: 45 | Teams: 120
ğŸ’¾ Checkpoint gespeichert
ğŸ“Š 200/2847 Ligen (7.0%) | Clubs: 89 | Teams: 245
ğŸ’¾ Checkpoint gespeichert
...
ğŸ“Š 2847/2847 Ligen (100.0%) | Clubs: 1250 | Teams: 3500
ğŸ’¾ Checkpoint gespeichert

âœ… Batch Processing abgeschlossen
   Erfolgreich: 2847/2847 Ligen

â­ï¸  Schritt 4: Kontaktdaten Ã¼bersprungen (--skip-kontakt)

ğŸ’¾ Schritt 5: Finale Speicherung...
ğŸ’¾ Backup: clubs-germany.backup-2025-10-22T16-30-00-000Z.json
âœ… Output: .../clubs-germany.json
   Clubs: 1250
   Teams: 3500
   Seasons: 5200
   VerbÃ¤nde: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 29, 30, 31, 32, 33, 100

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… BULK CRAWL ERFOLGREICH!
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â±ï¸  Dauer: 18.45 Minuten
ğŸ“Š Clubs: 1250
ğŸ“Š Teams (unique): 3500
ğŸ“ Output: .../clubs-germany.json
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ”„ Crash-Recovery

**Vorteil:** Periodic Flush alle 100 Ligen

**Szenario:**
1. Crawl lÃ¤uft
2. Bei Liga 1543 â†’ Crash (Internet weg)
3. â†’ `clubs-germany.json` hat 1500 Ligen
4. Neustart â†’ Fortsetzung ab Liga 1500

**Aktuell:** Kein Resume (Start von vorne)  
**ZukÃ¼nftig:** Resume-Logik mÃ¶glich

---

## ğŸ†š Wann welcher Crawler?

### Bulk (empfohlen fÃ¼r Production)

```bash
npm run crawl:clubs:bulk
```

**Wann:**
- âœ… Initial Crawl (alle VerbÃ¤nde)
- âœ… Kompletter Refresh
- âœ… GitHub Actions (monatlich/quartalsweise)
- âœ… Schnellste Option

**Dauer:** 15-25 min

---

### Incremental (fÃ¼r Development)

```bash
npm run crawl:clubs -- --verband=2
```

**Wann:**
- âœ… Single-Verband Update
- âœ… Testing einzelner VerbÃ¤nde
- âœ… Incremental Updates mit Cache

**Dauer:** 5-8 min pro Verband (mit Cache <1 sec)

---

### ALL (deprecated)

```bash
npm run crawl:clubs:all
```

**Status:** âš ï¸ Veraltet, nutze stattdessen `bulk`

**Wann:**
- âŒ Nicht mehr verwenden
- âœ… Wird durch `bulk` ersetzt

**Dauer:** 30-45 min (mit Cache)

---

## ğŸ¯ Empfohlener Workflow

### Production (GitHub Actions)

```bash
npm run crawl:clubs:bulk
npm run split:clubs  # (noch zu bauen)
```

**Cron:** Monatlich oder Quartalsweise

---

### Development

```bash
# Schneller Test
npm run crawl:clubs -- --verband=5 --skip-kontakt

# Kompletter Refresh
npm run crawl:clubs:bulk
```

---

## ğŸ› Troubleshooting

### "Rate limit" Fehler

**LÃ¶sung:** Script hat automatisches Retry mit Exponential Backoff

### Crawl dauert >30 Minuten

**MÃ¶gliche Ursachen:**
- API ist langsam
- Viele Kontaktdaten (ohne `--skip-kontakt`)
- Schlechte Internet-Verbindung

**LÃ¶sung:** Mit `--skip-kontakt` laufen lassen

### Crash Recovery

**Aktuell:** Neustart von vorne (letzte 100 Ligen verloren)  
**ZukÃ¼nftig:** Resume-Logik kann implementiert werden

---

## ğŸ“š Weitere Dokumentation

- **Performance-Details:** `docs/CRAWLER-OPTIMIZATIONS.md`
- **VerbÃ¤nde:** `docs/STATISCHE-VERBAENDE.md`
- **Automation:** `docs/CRAWLER-AUTOMATION.md`

---

## âœ… NÃ¤chste Schritte

```bash
# 1. Bulk Crawl starten
npm run crawl:clubs:bulk

# 2. Split in Chunks (noch zu bauen)
npm run split:clubs

# 3. App testen mit neuen Daten
npm run dev
```

---

**Version:** v1.0.0  
**Datum:** 2025-10-22  
**Status:** âœ… Production Ready
